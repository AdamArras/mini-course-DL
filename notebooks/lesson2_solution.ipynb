{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151a1fc1",
   "metadata": {},
   "source": [
    "# Lesson 2\n",
    "\n",
    "**Goal.** We will build and train a (very primitive) **character-level language model** on the dataset `names.txt`. The objective is to create a model that generates sequences of letters corresponding to new names. We will first construct a simple **count-based model** and evaluate it in terms of **MLE** (Maximum Likelihood Estimation). We will then train a **one-layer neural network** to become familiar with gradient descent.\n",
    "\n",
    "This lesson is based on **Andrej Karpathy’s “Neural Networks: Zero to Hero”** video series  \n",
    "(https://karpathy.ai/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fcc7d7-85ac-44eb-bba6-84cd0daccba0",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "- Dataset: list of names $w\\in\\mathcal{D}$, each a string over an alphabet $\\mathcal{A}$ including a special boundary token `\".\"`. This special characters stand for the beginng and the end of a name.\n",
    "- Vocabulary size: $V = |\\mathcal{A}| = 26+1 $.\n",
    "- Encode characters with a bijection:\n",
    "$$\n",
    "\\mathrm{stoi}:\\mathcal{A}\\to\\{0,\\dots,V-1\\},\\qquad \\mathrm{stoi}('.')=0,\n",
    "$$\n",
    "The name $\\mathrm{stoi}$ stands for string to interger, and we denote $\\mathrm{itos}=\\mathrm{stoi}^{-1}$. \n",
    "\n",
    "A **bigram model** assumes:\n",
    "$$\n",
    "P(c_t \\mid c_{t-1}, c_{t-2},\\dots) \\approx P(c_t \\mid c_{t-1}),\n",
    "$$\n",
    "i.e. a first-order Markov chain over characters.\n",
    "\n",
    "---\n",
    "\n",
    "## We will construct:\n",
    "\n",
    "1. The Bigram count matrix $N\\in\\mathbb{N}^{V\\times V}$\n",
    "2. The Bigram probability matrix $P\\in[0,1]^{V\\times V}$.\n",
    "3. Sampling procedure to generate new names\n",
    "4. Negative log-likelihood (NLL) evaluation of the model\n",
    "5. Neural formulation: one-hot input $\\to$ linear logits $\\to$ softmax $\\to$ NLL, trained by gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37eb7847-92ed-4e63-a840-6f2d721bfb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content'\n",
      "/home/adam/Desktop/Learning/nn-zero_to_hero/my course/mini-course-DL\n",
      "Cloning into 'mini-course-DL'...\n",
      "remote: Enumerating objects: 27, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 27 (delta 3), reused 27 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (27/27), 136.37 KiB | 924.00 KiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n",
      "/home/adam/Desktop/Learning/nn-zero_to_hero/my course/mini-course-DL/mini-course-DL\n"
     ]
    }
   ],
   "source": [
    "# if running on google colab, run this cell\n",
    "%cd /content\n",
    "!rm -rf mini-course-DL\n",
    "!git clone https://github.com/AdamArras/mini-course-DL.git\n",
    "%cd mini-course-DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a29cfa5-3c5a-465f-8c30-d3fb0c2cd392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4c980",
   "metadata": {},
   "source": [
    "## 1) Setup and load dataset\n",
    "\n",
    "**Exercise.**\n",
    "1. Load `names.txt` into a list `words`.\n",
    "2. Display: \n",
    "   - the 10 first names of the dataset.\n",
    "   - the total number of neames.\n",
    "   - the size of the shorted and the longuest name.\n",
    "\n",
    "remark : if you have problem with finding the path for the files, do the comand `%ls` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8621839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: imports + load words + sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324b066c-7e4e-471f-b21d-289a3a1223fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "32033\n",
      "min lenght = 2 and max lenght = 15\n"
     ]
    }
   ],
   "source": [
    "file = open('data/names.txt', 'r') \n",
    "words = file.read().splitlines() \n",
    "file.close() \n",
    "\n",
    "print(type(words))\n",
    "print(words[:10])\n",
    "print(len(words))\n",
    "print('min lenght = '+str(min([len(w) for w in words]))+ ' and max lenght = ' +str(max([len(w) for w in words])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37ae57",
   "metadata": {},
   "source": [
    "## 2) Build vocabulary and integer encoding\n",
    "\n",
    "**Exercise.**\n",
    "1. Collect characters:\n",
    "$$\n",
    "\\mathcal{A} = \\text{sorted unique characters in the dataset}.\n",
    "$$\n",
    "2. Build dictionaries `stoi`, `itos` with `\".\"` mapped to 0.\n",
    "3. Define `vocab_size = V`.\n",
    "\n",
    "**Check.** Print `itos` and `vocab_size`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "917e792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build chars, stoi, itos, vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5c6f6bd-a3f1-44b9-ab12-d403f773fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = str('.abcdefghijklmnopqrstuvwxyz')\n",
    "vocab_size = len(alphabet)\n",
    "itos = {i:alphabet[i] for i in range(vocab_size)}\n",
    "stoi = {alphabet[i]:i for i in range(vocab_size)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5e31d7",
   "metadata": {},
   "source": [
    "## 3) Bigram counts $N$\n",
    "\n",
    "For each word $w = w_1\\cdots w_L$, form the augmented sequence:\n",
    "$$\n",
    "(. , w_1, w_2, \\dots, w_L, .)\n",
    "$$\n",
    "where $.$ is our special characters, and count transitions $(c_{\\text{prev}}\\to c_{\\text{next}})$.\n",
    "\n",
    "Define:\n",
    "$$\n",
    "N_{i,j} = \\#\\{ \\text{times character } i \\text{ is followed by character } j\\}.\n",
    "$$\n",
    "\n",
    "**Exercise.**\n",
    "1. Initialize $N\\in\\mathbb{N}^{V\\times V}$ with zeros (integer dtype).\n",
    "2. Loop over all words and all adjacent pairs (including boundary `\".\"`).\n",
    "3. Increment the appropriate cell in $N$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f2e160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build N (VxV) bigram count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e90d79b6-430c-4293-85ae-557705350c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((vocab_size,vocab_size),dtype =torch.int32)\n",
    "for names in words:\n",
    "    augmented_name = '.'+str(names)+'.'\n",
    "    for ch1,ch2 in zip(augmented_name,augmented_name[1:]):\n",
    "        i1,i2 = stoi[ch1],stoi[ch2]\n",
    "        N[i1,i2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1020536",
   "metadata": {},
   "source": [
    "## 4) Visualize $N$\n",
    "\n",
    "**Exercise.**\n",
    "- Plot $N$ as an image (e.g. `imshow`).\n",
    "- Overlay:\n",
    "  - the bigram label `itos[i] + itos[j]`\n",
    "  - the count $N_{i,j}$\n",
    "\n",
    "This lets you visually inspect which transitions are frequent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87e22f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: visualize N with matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b3607f4-a0c2-4a8c-bebe-1946239ff56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.YTick at 0x7997e3442960>,\n",
       "  <matplotlib.axis.YTick at 0x7997e3442840>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34c7c20>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34c6180>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34d8680>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34d90d0>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34d9a90>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34da4e0>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34a8560>,\n",
       "  <matplotlib.axis.YTick at 0x7996d2972600>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34daa80>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34db4a0>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34dbe60>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34dacf0>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34f8770>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34f9130>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34f9b20>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34fa510>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34fa7e0>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34fade0>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34fb800>,\n",
       "  <matplotlib.axis.YTick at 0x7997e3308260>,\n",
       "  <matplotlib.axis.YTick at 0x7997e3308c20>,\n",
       "  <matplotlib.axis.YTick at 0x7997e34fb050>,\n",
       "  <matplotlib.axis.YTick at 0x7997e3309580>,\n",
       "  <matplotlib.axis.YTick at 0x7997e3309f70>,\n",
       "  <matplotlib.axis.YTick at 0x7997e330a960>],\n",
       " [Text(0, 0, '.'),\n",
       "  Text(0, 1, 'a'),\n",
       "  Text(0, 2, 'b'),\n",
       "  Text(0, 3, 'c'),\n",
       "  Text(0, 4, 'd'),\n",
       "  Text(0, 5, 'e'),\n",
       "  Text(0, 6, 'f'),\n",
       "  Text(0, 7, 'g'),\n",
       "  Text(0, 8, 'h'),\n",
       "  Text(0, 9, 'i'),\n",
       "  Text(0, 10, 'j'),\n",
       "  Text(0, 11, 'k'),\n",
       "  Text(0, 12, 'l'),\n",
       "  Text(0, 13, 'm'),\n",
       "  Text(0, 14, 'n'),\n",
       "  Text(0, 15, 'o'),\n",
       "  Text(0, 16, 'p'),\n",
       "  Text(0, 17, 'q'),\n",
       "  Text(0, 18, 'r'),\n",
       "  Text(0, 19, 's'),\n",
       "  Text(0, 20, 't'),\n",
       "  Text(0, 21, 'u'),\n",
       "  Text(0, 22, 'v'),\n",
       "  Text(0, 23, 'w'),\n",
       "  Text(0, 24, 'x'),\n",
       "  Text(0, 25, 'y'),\n",
       "  Text(0, 26, 'z')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(N, cmap = 'grey')\n",
    "plt.xticks(range(27), [itos[i] for i in range(27)])\n",
    "plt.yticks(range(27), [itos[i] for i in range(27)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a3555",
   "metadata": {},
   "source": [
    "## 5) Convert counts to probabilities $P$ \n",
    "\n",
    "Raw MLE would be:\n",
    "$$\n",
    "P_{i,j} = \\frac{N_{i,j}}{\\sum_{k} N_{i,k}}.\n",
    "$$\n",
    "\n",
    "We use **Laplace smoothing** to avoid zeros:\n",
    "$$\n",
    "P_{i,j} = \\frac{N_{i,j} + 1}{\\sum_k (N_{i,k}+1)}.\n",
    "$$\n",
    "\n",
    "**Exercise.**\n",
    "1. Construct $P$ as float.\n",
    "2. Normalize each row so it sums to 1:\n",
    "$$\n",
    "\\sum_j P_{i,j} = 1\\ \\text{for all } i.\n",
    "$$\n",
    "3. Verify row sums are ~1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a424a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute P from N using Laplace smoothingP = torch.zeros_like(N, dtype = float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b4449c3-290a-4f05-bc45-c0aed8bed391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = torch.zeros_like(N,dtype = float)\n",
    "P += N + 1.0\n",
    "P = P / P.sum(1, keepdim=True)\n",
    "sum(P[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ffd950",
   "metadata": {},
   "source": [
    "## 6) Sampling from the bigram model\n",
    "\n",
    "We generate names by sampling a Markov chain:\n",
    "\n",
    "Initialize $c_0 = '.'$ (index 0). Then iterate:\n",
    "$$\n",
    "c_{t} \\sim \\mathrm{Categorical}(P_{c_{t-1}, :})\n",
    "$$\n",
    "until $c_t='.'$ again, then stop.\n",
    "\n",
    "**Exercise.**\n",
    "- Write a loop that generates, say, 5–20 names.\n",
    "- Use a fixed `torch.Generator().manual_seed(...)` for reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7a62296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sample names from P (bigram Markov chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eae32404-9c8d-4db0-a8db-e894072ae6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anugeenvi\n",
      "s\n",
      "mabidushan\n",
      "stan\n",
      "silaylelaremah\n",
      "li\n",
      "le\n",
      "epiachalen\n",
      "diza\n",
      "k\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "\n",
    "nb_names = 10\n",
    "for nb in range(nb_names):\n",
    "    name = []\n",
    "    it = 0\n",
    "    while True:\n",
    "        p = P[it,:]\n",
    "        it = torch.multinomial(p,num_samples=1,generator = g).item()\n",
    "        ch = itos[it]\n",
    "        if it == 0:\n",
    "            break\n",
    "        name.append(ch)\n",
    "    print(\"\".join(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1415673d",
   "metadata": {},
   "source": [
    "## 7) Evaluate model: log-likelihood.\n",
    "\n",
    "For each observed bigram $(i\\to j)$, the model assigns probability $P_{i,j}$.\n",
    "\n",
    "Total log-likelihood over the dataset:\n",
    "$$\n",
    "\\log \\mathcal{L} = \\sum_{(i\\to j)\\ \\text{in data}} \\log P_{i,j}.\n",
    "$$\n",
    "\n",
    "Negative log-likelihood (NLL) per bigram:\n",
    "$$\n",
    "\\mathrm{NLL} = -\\frac{1}{N_{\\text{bigrams}}}\\sum \\log P_{i,j}.\n",
    "$$\n",
    "\n",
    "**Exercise.**\n",
    "- Compute total log-likelihood and average NLL.\n",
    "- (Optional) Print a few bigrams and their probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db3bf3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute log-likelihood and NLL for the dataset under P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "faed563c-77e4-403c-a4fd-fe1a4e7b9861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.454576820116076\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "count = 0\n",
    "\n",
    "for word in words:\n",
    "    augmented = \".\" + word + \".\"\n",
    "    for ch1, ch2 in zip(augmented, augmented[1:]):\n",
    "        i = stoi[ch1]\n",
    "        j = stoi[ch2]\n",
    "        log_likelihood += torch.log(P[i, j])\n",
    "        count += 1\n",
    "\n",
    "nll = -log_likelihood / count\n",
    "print(nll.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472438d7",
   "metadata": {},
   "source": [
    "## 8) Neural formulation of the bigram model (conceptual step)\n",
    "\n",
    "In this part, we reformulate the bigram model as a **neural network**.\n",
    "Nothing fundamentally new is learned yet, the goal is to understand how\n",
    "**counting → probabilities** can be replaced by **parameters → optimization**.\n",
    "\n",
    "### Mathematical model\n",
    "\n",
    "Let:\n",
    "- Vocabulary size: $V$\n",
    "- Training bigrams: $(x_t, y_t)$, where  \n",
    "  $x_t \\in {0,\\ldots,V−1}$ is the previous character index,  \n",
    "  $y_t \\in \\{0,\\ldots,V−1\\}$ is the next character index.\n",
    "\n",
    "We introduce a parameter matrix:\n",
    "$W \\in \\mathbb{R}^{V×V}$.\n",
    "\n",
    "Interpretation:\n",
    "- Row $i$ of $W$ contains the logits for predicting the next character,\n",
    "  given that the previous character index is $i$.\n",
    "\n",
    "### One-hot encoding\n",
    "\n",
    "For each input index $x_t \\in \\{0,\\ldots,V−1\\}$, define its one-hot vector: $onehot(x_t) \\in \\mathbb{R}^V$ to be the associated vector basis in $\\mathbb{R}^{\\{0,\\ldots,V−1\\}} \\simeq \\mathbb{R}^V$.\n",
    "\n",
    "Stacking all examples gives:\n",
    "$X ∈ \\mathbb{R}^{N×V}.$\n",
    "\n",
    "### Forward pass\n",
    "\n",
    "For each example:\n",
    "$l_t = onehot(x_t) · W \\in \\mathbb{R}^V.$ (Here $l$ stands for logits).\n",
    "\n",
    "Convert logits to probabilities using softmax:\n",
    "$$p_{t,j} = \\frac{ \\exp(l_{t,j}) }{ \\sum_k exp(l_{t,k})}$$\n",
    "\n",
    "This models:\n",
    "$p_{t,j} ≈ P(y_t = j | x_t).$\n",
    "\n",
    "### Loss (Negative Log-Likelihood)\n",
    "\n",
    "We train by minimizing the negative log-likelihood (NLL):\n",
    "$$L_{NLL} = \\frac{-1}{N}  \\sum_t \\log p_{t, y_t}.$$\n",
    "\n",
    "This is exactly maximum likelihood estimation for the bigram model,\n",
    "written in differentiable, parametric form.\n",
    "\n",
    "At this stage, **do not add regularization yet**.\n",
    "The focus is understanding logits, softmax, and NLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af488445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build xs, ys over the full dataset of bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19e22644-703c-473d-8ddd-ba09d38b328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs,ys = [],[]\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1,ch2 in zip(chs,chs[1:]):\n",
    "        it1,it2 = stoi[ch1],stoi[ch2]\n",
    "        xs.append(it1)\n",
    "        ys.append(it2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "X = F.one_hot(xs, num_classes = vocab_size).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1626442",
   "metadata": {},
   "source": [
    "## 9) Gradient descent training loop (optimization mechanics)\n",
    "\n",
    "We now explain how PyTorch optimizes the parameter matrix\n",
    "$$\n",
    "W \\in \\mathbb{R}^{V \\times V}.\n",
    "$$\n",
    "\n",
    "### Optimization objective\n",
    "\n",
    "We minimize the **negative log-likelihood (NLL)**, with\n",
    "quadratic ($L^2$ / Frobenius) regularization:\n",
    "$$\n",
    "\\mathcal{L}\n",
    "=\n",
    "\\mathcal{L}_{\\text{NLL}}\n",
    "+\n",
    "\\lambda \\|W\\|_F^2,\n",
    "\\qquad\n",
    "\\|W\\|_F^2 = \\sum_{i,j} W_{i,j}^2.\n",
    "$$\n",
    "\n",
    "- Setting $\\lambda = 0$ corresponds to **pure maximum likelihood estimation (MLE)**.\n",
    "- Introduce $\\lambda > 0$ only after the basic training loop works.\n",
    "\n",
    "### How gradients work in PyTorch\n",
    "\n",
    "- Tensors created with `requires_grad=True` record their computation graph.\n",
    "- Calling `loss.backward()` applies **reverse-mode automatic differentiation**.\n",
    "- After the backward pass, `W.grad` stores the gradient:\n",
    "$$\n",
    "W.\\text{grad} = \\nabla_W \\mathcal{L}.\n",
    "$$\n",
    "- Gradients **accumulate across iterations**, so they must be reset manually\n",
    "  at each training step.\n",
    "\n",
    "### Gradient descent update\n",
    "\n",
    "With learning rate $\\eta > 0$, parameters are updated according to:\n",
    "$$\n",
    "W \\leftarrow W - \\eta \\nabla_W \\mathcal{L}.\n",
    "$$\n",
    "\n",
    "In PyTorch, this update must be performed **without tracking gradients**\n",
    "(e.g. inside a `torch.no_grad()` block).\n",
    "\n",
    "### Training loop structure\n",
    "\n",
    "Each training iteration must contain the following four steps:\n",
    "\n",
    "1. **Forward pass**\n",
    "   - Compute logits: $\\text{logits} = XW$\n",
    "   - Compute the loss (NLL, optionally plus regularization)\n",
    "\n",
    "2. **Backward pass**\n",
    "   - Reset stored gradients\n",
    "   - Call `loss.backward()` to compute gradients\n",
    "\n",
    "3. **Parameter update**\n",
    "   - Update $W$ using gradient descent\n",
    "   - Do not track gradients during the update\n",
    "\n",
    "4. **Monitoring**\n",
    "   - Record the loss value to verify that it decreases over iterations\n",
    "\n",
    "### Goal of this section\n",
    "\n",
    "Understand explicitly:\n",
    "- how probabilistic models become **trainable neural models**,\n",
    "- where gradients are stored,\n",
    "- and how parameters are updated by gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbba6f5-46a0-4d36-a8a5-b70831099a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: built the Gradient descent training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a43120c-76ce-4bdf-8f0c-8fccf33d2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0/1000  loss=3.7163  nll=3.6966  reg=0.0197\n",
      " 100/1000  loss=2.5870  nll=2.5673  reg=0.0197\n",
      " 200/1000  loss=2.5331  nll=2.5101  reg=0.0230\n",
      " 300/1000  loss=2.5165  nll=2.4913  reg=0.0252\n",
      " 400/1000  loss=2.5092  nll=2.4825  reg=0.0268\n",
      " 500/1000  loss=2.5054  nll=2.4775  reg=0.0279\n",
      " 600/1000  loss=2.5032  nll=2.4745  reg=0.0288\n",
      " 700/1000  loss=2.5018  nll=2.4724  reg=0.0294\n",
      " 800/1000  loss=2.5009  nll=2.4709  reg=0.0299\n",
      " 900/1000  loss=2.5002  nll=2.4699  reg=0.0303\n"
     ]
    }
   ],
   "source": [
    "K = 1000\n",
    "eta = 10.0 # this is a big value in practice we will see eta of oder 10^{-1}\n",
    "lam = 0.02\n",
    "\n",
    "loss_hist = []\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "W = torch.randn((vocab_size, vocab_size), generator=g, requires_grad=True)\n",
    "\n",
    "for k in range(K):\n",
    "    # 1) forward: logits + loss\n",
    "    logits = X @ W\n",
    "    nll = F.cross_entropy(logits, ys)          \n",
    "    reg = lam * (W**2).mean()                \n",
    "    loss = nll + reg\n",
    "\n",
    "    # 2) backward: zero grads + backprop\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # 3) update: gradient descent step (no grad tracking)\n",
    "    with torch.no_grad():\n",
    "        W -= eta * W.grad\n",
    "\n",
    "    # 4) monitor\n",
    "    loss_hist.append(loss.item())\n",
    "    if k % 100 == 0:\n",
    "        print(f\"{k:4d}/{K}  loss={loss.item():.4f}  nll={nll.item():.4f}  reg={reg.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad718d24-d464-4b8a-8241-a618610b8f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_learned = torch.softmax(W, dim=1)\n",
    "\n",
    "P_learned.sum(1)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "745ed1ef-e01c-4c85-bf56-bb810ced5d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.3609e-02, 2.3610e-01, 3.6212e-04, 7.2424e-04, 2.7159e-02, 2.3248e-01,\n",
      "        1.0864e-03, 4.7076e-03, 2.1546e-02, 1.2222e-01, 1.8106e-03, 7.2424e-04,\n",
      "        1.1045e-02, 5.6129e-03, 5.7940e-03, 6.8622e-02, 1.8106e-04, 3.6212e-04,\n",
      "        7.6951e-02, 5.4318e-03, 9.0531e-04, 1.6839e-02, 3.2591e-03, 4.3455e-03,\n",
      "        1.8106e-04, 5.7577e-02, 3.6212e-04], dtype=torch.float64)\n",
      "tensor([0.0890, 0.2301, 0.0037, 0.0037, 0.0248, 0.2265, 0.0037, 0.0063, 0.0198,\n",
      "        0.1172, 0.0042, 0.0041, 0.0106, 0.0067, 0.0068, 0.0646, 0.0040, 0.0039,\n",
      "        0.0727, 0.0067, 0.0045, 0.0157, 0.0041, 0.0050, 0.0036, 0.0539, 0.0040],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(P[4,:])\n",
    "print(P_learned[4,:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nn-zero-to-hero)",
   "language": "python",
   "name": "nn-zero-to-hero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
