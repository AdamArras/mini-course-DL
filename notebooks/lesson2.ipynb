{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151a1fc1",
   "metadata": {},
   "source": [
    "# Lesson 2\n",
    "\n",
    "**Goal.** We will build and train a (very primitive) **character-level language model** on the dataset `names.txt`. The objective is to create a model that generates sequences of letters corresponding to new names. We will first construct a simple **count-based model** and evaluate it in terms of **MLE** (Maximum Likelihood Estimation). We will then train a **one-layer neural network** to become familiar with gradient descent.\n",
    "\n",
    "This lesson is based on **Andrej Karpathy’s “Neural Networks: Zero to Hero”** video series  \n",
    "(https://karpathy.ai/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fcc7d7-85ac-44eb-bba6-84cd0daccba0",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "- Dataset: list of names $w\\in\\mathcal{D}$, each a string over an alphabet $\\mathcal{A}$ including a special boundary token `\".\"`. This special characters stand for the beginng and the end of a name.\n",
    "- Vocabulary size: $V = |\\mathcal{A}| = 26+1 $.\n",
    "- Encode characters with a bijection:\n",
    "$$\n",
    "\\mathrm{stoi}:\\mathcal{A}\\to\\{0,\\dots,V-1\\},\\qquad \\mathrm{stoi}('.')=0,\n",
    "$$\n",
    "The name $\\mathrm{stoi}$ stands for string to interger, and we denote $\\mathrm{itos}=\\mathrm{stoi}^{-1}$. \n",
    "\n",
    "A **bigram model** assumes:\n",
    "$$\n",
    "P(c_t \\mid c_{t-1}, c_{t-2},\\dots) \\approx P(c_t \\mid c_{t-1}),\n",
    "$$\n",
    "i.e. a first-order Markov chain over characters.\n",
    "\n",
    "---\n",
    "\n",
    "## We will construct:\n",
    "\n",
    "1. The Bigram count matrix $N\\in\\mathbb{N}^{V\\times V}$\n",
    "2. The Bigram probability matrix $P\\in[0,1]^{V\\times V}$.\n",
    "3. Sampling procedure to generate new names\n",
    "4. Negative log-likelihood (NLL) evaluation of the model\n",
    "5. Neural formulation: one-hot input $\\to$ linear logits $\\to$ softmax $\\to$ NLL, trained by gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37eb7847-92ed-4e63-a840-6f2d721bfb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content'\n",
      "/home/adam/Desktop/Learning/nn-zero_to_hero/my course/mini-course-DL\n",
      "Cloning into 'mini-course-DL'...\n",
      "remote: Enumerating objects: 27, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 27 (delta 3), reused 27 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (27/27), 136.37 KiB | 924.00 KiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n",
      "/home/adam/Desktop/Learning/nn-zero_to_hero/my course/mini-course-DL/mini-course-DL\n"
     ]
    }
   ],
   "source": [
    "# if running on google colab, run this cell\n",
    "%cd /content\n",
    "!rm -rf mini-course-DL\n",
    "!git clone https://github.com/AdamArras/mini-course-DL.git\n",
    "%cd mini-course-DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a29cfa5-3c5a-465f-8c30-d3fb0c2cd392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4c980",
   "metadata": {},
   "source": [
    "## 1) Setup and load dataset\n",
    "\n",
    "**Exercise.**\n",
    "1. Load `names.txt` into a list `words`.\n",
    "2. Display: \n",
    "   - the 10 first names of the dataset.\n",
    "   - the total number of neames.\n",
    "   - the size of the shorted and the longuest name.\n",
    "\n",
    "remark : if you have problem with finding the path for the files, do the comand `%ls` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8621839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: imports + load words + sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37ae57",
   "metadata": {},
   "source": [
    "## 2) Build vocabulary and integer encoding\n",
    "\n",
    "**Exercise.**\n",
    "1. Collect characters:\n",
    "$$\n",
    "\\mathcal{A} = \\text{sorted unique characters in the dataset}.\n",
    "$$\n",
    "2. Build dictionaries `stoi`, `itos` with `\".\"` mapped to 0.\n",
    "3. Define `vocab_size = V`.\n",
    "\n",
    "**Check.** Print `itos` and `vocab_size`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "917e792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build chars, stoi, itos, vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5e31d7",
   "metadata": {},
   "source": [
    "## 3) Bigram counts $N$\n",
    "\n",
    "For each word $w = w_1\\cdots w_L$, form the augmented sequence:\n",
    "$$\n",
    "(. , w_1, w_2, \\dots, w_L, .)\n",
    "$$\n",
    "where $.$ is our special characters, and count transitions $(c_{\\text{prev}}\\to c_{\\text{next}})$.\n",
    "\n",
    "Define:\n",
    "$$\n",
    "N_{i,j} = \\#\\{ \\text{times character } i \\text{ is followed by character } j\\}.\n",
    "$$\n",
    "\n",
    "**Exercise.**\n",
    "1. Initialize $N\\in\\mathbb{N}^{V\\times V}$ with zeros (integer dtype).\n",
    "2. Loop over all words and all adjacent pairs (including boundary `\".\"`).\n",
    "3. Increment the appropriate cell in $N$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f2e160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build N (VxV) bigram count matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1020536",
   "metadata": {},
   "source": [
    "## 4) Visualize $N$\n",
    "\n",
    "**Exercise.**\n",
    "- Plot $N$ as an image (e.g. `imshow`).\n",
    "- Overlay:\n",
    "  - the bigram label `itos[i] + itos[j]`\n",
    "  - the count $N_{i,j}$\n",
    "\n",
    "This lets you visually inspect which transitions are frequent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87e22f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: visualize N with matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a3555",
   "metadata": {},
   "source": [
    "## 5) Convert counts to probabilities $P$ \n",
    "\n",
    "Raw MLE would be:\n",
    "$$\n",
    "P_{i,j} = \\frac{N_{i,j}}{\\sum_{k} N_{i,k}}.\n",
    "$$\n",
    "\n",
    "We use **Laplace smoothing** to avoid zeros:\n",
    "$$\n",
    "P_{i,j} = \\frac{N_{i,j} + 1}{\\sum_k (N_{i,k}+1)}.\n",
    "$$\n",
    "\n",
    "**Exercise.**\n",
    "1. Construct $P$ as float.\n",
    "2. Normalize each row so it sums to 1:\n",
    "$$\n",
    "\\sum_j P_{i,j} = 1\\ \\text{for all } i.\n",
    "$$\n",
    "3. Verify row sums are ~1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a424a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute P from N using Laplace smoothingP = torch.zeros_like(N, dtype = float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ffd950",
   "metadata": {},
   "source": [
    "## 6) Sampling from the bigram model\n",
    "\n",
    "We generate names by sampling a Markov chain:\n",
    "\n",
    "Initialize $c_0 = '.'$ (index 0). Then iterate:\n",
    "$$\n",
    "c_{t} \\sim \\mathrm{Categorical}(P_{c_{t-1}, :})\n",
    "$$\n",
    "until $c_t='.'$ again, then stop.\n",
    "\n",
    "**Exercise.**\n",
    "- Write a loop that generates, say, 5–20 names.\n",
    "- Use a fixed `torch.Generator().manual_seed(...)` for reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7a62296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sample names from P (bigram Markov chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1415673d",
   "metadata": {},
   "source": [
    "## 7) Evaluate model: log-likelihood.\n",
    "\n",
    "For each observed bigram $(i\\to j)$, the model assigns probability $P_{i,j}$.\n",
    "\n",
    "Total log-likelihood over the dataset:\n",
    "$$\n",
    "\\log \\mathcal{L} = \\sum_{(i\\to j)\\ \\text{in data}} \\log P_{i,j}.\n",
    "$$\n",
    "\n",
    "Negative log-likelihood (NLL) per bigram:\n",
    "$$\n",
    "\\mathrm{NLL} = -\\frac{1}{N_{\\text{bigrams}}}\\sum \\log P_{i,j}.\n",
    "$$\n",
    "\n",
    "**Exercise.**\n",
    "- Compute total log-likelihood and average NLL.\n",
    "- (Optional) Print a few bigrams and their probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db3bf3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute log-likelihood and NLL for the dataset under P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472438d7",
   "metadata": {},
   "source": [
    "## 8) Neural formulation of the bigram model (conceptual step)\n",
    "\n",
    "In this part, we reformulate the bigram model as a **neural network**.\n",
    "Nothing fundamentally new is learned yet, the goal is to understand how\n",
    "**counting → probabilities** can be replaced by **parameters → optimization**.\n",
    "\n",
    "### Mathematical model\n",
    "\n",
    "Let:\n",
    "- Vocabulary size: $V$\n",
    "- Training bigrams: $(x_t, y_t)$, where  \n",
    "  $x_t \\in {0,\\ldots,V−1}$ is the previous character index,  \n",
    "  $y_t \\in \\{0,\\ldots,V−1\\}$ is the next character index.\n",
    "\n",
    "We introduce a parameter matrix:\n",
    "$W \\in \\mathbb{R}^{V×V}$.\n",
    "\n",
    "Interpretation:\n",
    "- Row $i$ of $W$ contains the logits for predicting the next character,\n",
    "  given that the previous character index is $i$.\n",
    "\n",
    "### One-hot encoding\n",
    "\n",
    "For each input index $x_t \\in \\{0,\\ldots,V−1\\}$, define its one-hot vector: $onehot(x_t) \\in \\mathbb{R}^V$ to be the associated vector basis in $\\mathbb{R}^{\\{0,\\ldots,V−1\\}} \\simeq \\mathbb{R}^V$.\n",
    "\n",
    "Stacking all examples gives:\n",
    "$X ∈ \\mathbb{R}^{N×V}.$\n",
    "\n",
    "### Forward pass\n",
    "\n",
    "For each example:\n",
    "$l_t = onehot(x_t) · W \\in \\mathbb{R}^V.$ (Here $l$ stands for logits).\n",
    "\n",
    "Convert logits to probabilities using softmax:\n",
    "$$p_{t,j} = \\frac{ \\exp(l_{t,j}) }{ \\sum_k exp(l_{t,k})}$$\n",
    "\n",
    "This models:\n",
    "$p_{t,j} ≈ P(y_t = j | x_t).$\n",
    "\n",
    "### Loss (Negative Log-Likelihood)\n",
    "\n",
    "We train by minimizing the negative log-likelihood (NLL):\n",
    "$$L_{NLL} = \\frac{-1}{N}  \\sum_t \\log p_{t, y_t}.$$\n",
    "\n",
    "This is exactly maximum likelihood estimation for the bigram model,\n",
    "written in differentiable, parametric form.\n",
    "\n",
    "At this stage, **do not add regularization yet**.\n",
    "The focus is understanding logits, softmax, and NLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af488445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build xs, ys over the full dataset of bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1626442",
   "metadata": {},
   "source": [
    "## 9) Gradient descent training loop (optimization mechanics)\n",
    "\n",
    "We now explain how PyTorch optimizes the parameter matrix\n",
    "$$\n",
    "W \\in \\mathbb{R}^{V \\times V}.\n",
    "$$\n",
    "\n",
    "### Optimization objective\n",
    "\n",
    "We minimize the **negative log-likelihood (NLL)**, with\n",
    "quadratic ($L^2$ / Frobenius) regularization:\n",
    "$$\n",
    "\\mathcal{L}\n",
    "=\n",
    "\\mathcal{L}_{\\text{NLL}}\n",
    "+\n",
    "\\lambda \\|W\\|_F^2,\n",
    "\\qquad\n",
    "\\|W\\|_F^2 = \\sum_{i,j} W_{i,j}^2.\n",
    "$$\n",
    "\n",
    "- Setting $\\lambda = 0$ corresponds to **pure maximum likelihood estimation (MLE)**.\n",
    "- Introduce $\\lambda > 0$ only after the basic training loop works.\n",
    "\n",
    "### How gradients work in PyTorch\n",
    "\n",
    "- Tensors created with `requires_grad=True` record their computation graph.\n",
    "- Calling `loss.backward()` applies **reverse-mode automatic differentiation**.\n",
    "- After the backward pass, `W.grad` stores the gradient:\n",
    "$$\n",
    "W.\\text{grad} = \\nabla_W \\mathcal{L}.\n",
    "$$\n",
    "- Gradients **accumulate across iterations**, so they must be reset manually\n",
    "  at each training step.\n",
    "\n",
    "### Gradient descent update\n",
    "\n",
    "With learning rate $\\eta > 0$, parameters are updated according to:\n",
    "$$\n",
    "W \\leftarrow W - \\eta \\nabla_W \\mathcal{L}.\n",
    "$$\n",
    "\n",
    "In PyTorch, this update must be performed **without tracking gradients**\n",
    "(e.g. inside a `torch.no_grad()` block).\n",
    "\n",
    "### Training loop structure\n",
    "\n",
    "Each training iteration must contain the following four steps:\n",
    "\n",
    "1. **Forward pass**\n",
    "   - Compute logits: $\\text{logits} = XW$\n",
    "   - Compute the loss (NLL, optionally plus regularization)\n",
    "\n",
    "2. **Backward pass**\n",
    "   - Reset stored gradients\n",
    "   - Call `loss.backward()` to compute gradients\n",
    "\n",
    "3. **Parameter update**\n",
    "   - Update $W$ using gradient descent\n",
    "   - Do not track gradients during the update\n",
    "\n",
    "4. **Monitoring**\n",
    "   - Record the loss value to verify that it decreases over iterations\n",
    "\n",
    "### Goal of this section\n",
    "\n",
    "Understand explicitly:\n",
    "- how probabilistic models become **trainable neural models**,\n",
    "- where gradients are stored,\n",
    "- and how parameters are updated by gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8476f-9c94-4536-8370-ce3ab3fbd4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: built the Gradient descent training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad718d24-d464-4b8a-8241-a618610b8f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_learned = torch.softmax(W, dim=1)\n",
    "\n",
    "P_learned.sum(1)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "745ed1ef-e01c-4c85-bf56-bb810ced5d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.3609e-02, 2.3610e-01, 3.6212e-04, 7.2424e-04, 2.7159e-02, 2.3248e-01,\n",
      "        1.0864e-03, 4.7076e-03, 2.1546e-02, 1.2222e-01, 1.8106e-03, 7.2424e-04,\n",
      "        1.1045e-02, 5.6129e-03, 5.7940e-03, 6.8622e-02, 1.8106e-04, 3.6212e-04,\n",
      "        7.6951e-02, 5.4318e-03, 9.0531e-04, 1.6839e-02, 3.2591e-03, 4.3455e-03,\n",
      "        1.8106e-04, 5.7577e-02, 3.6212e-04], dtype=torch.float64)\n",
      "tensor([0.0890, 0.2301, 0.0037, 0.0037, 0.0248, 0.2265, 0.0037, 0.0063, 0.0198,\n",
      "        0.1172, 0.0042, 0.0041, 0.0106, 0.0067, 0.0068, 0.0646, 0.0040, 0.0039,\n",
      "        0.0727, 0.0067, 0.0045, 0.0157, 0.0041, 0.0050, 0.0036, 0.0539, 0.0040],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(P[4,:])\n",
    "print(P_learned[4,:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nn-zero-to-hero)",
   "language": "python",
   "name": "nn-zero-to-hero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
