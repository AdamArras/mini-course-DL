{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a265521e-aa86-4fe9-b86c-a488a921ef2f",
   "metadata": {},
   "source": [
    "# Lesson 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f6acd",
   "metadata": {},
   "source": [
    "**Goal.** We upgrade our previous model to a **fixed-context neural language model**\n",
    "(a neural $n$-gram). We increase the context length $T$, we introduce the embedding for characters and a multi-layer perceptron that predicts will the next character from the context.\n",
    "\n",
    "This lesson is based on Andrej Karpathyâ€™s *Neural Networks: Zero to Hero* video series  \n",
    "(https://karpathy.ai/), and follows the MLP architecture as introduced in the foundational paper  \n",
    "**Bengio et al. (2003)**, *A Neural Probabilistic Language Model*  \n",
    "(https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27222550-a7c8-4914-9add-2e4e60ff8e8e",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "- Vocabulary size: $V = 26 + 1$.\n",
    "- Context length: $T = 3$.\n",
    "- Embedding dimension: $d$.\n",
    "- Hidden width: $H$.\n",
    "\n",
    "A single training example consists of:\n",
    "$$\n",
    "x = (x_1,\\dots,x_T)\\in\\{0,\\dots,V-1\\}^T,\\qquad\n",
    "y\\in\\{0,\\dots,V-1\\}.\n",
    "$$\n",
    "\n",
    "We model the conditional distribution of the next character as:\n",
    "$$\n",
    "p_\\theta(y \\mid x)\n",
    "=\n",
    "\\mathrm{softmax}(\\text{logits}(x))_y.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture\n",
    "\n",
    "### 1) Embedding lookup\n",
    "\n",
    "Embedding table:\n",
    "$$\n",
    "C \\in \\mathbb{R}^{V \\times d}.\n",
    "$$\n",
    "\n",
    "For a batch of size $n$, that is, $X \\in \\mathbb{Z}^{n \\times T}$:\n",
    "$$\n",
    "E = C[X] \\in \\mathbb{R}^{n \\times T \\times d}.\n",
    "$$\n",
    "\n",
    "Concatenating the embeddings along the context dimension gives:\n",
    "$$\n",
    "\\mathrm{cat}(E) \\in \\mathbb{R}^{n \\times (T d)}.\n",
    "$$\n",
    "\n",
    "### 2) MLP\n",
    "\n",
    "Parameters:\n",
    "$$\n",
    "W_1 \\in \\mathbb{R}^{(T d) \\times H},\\quad\n",
    "b_1 \\in \\mathbb{R}^{H},\\qquad\n",
    "W_2 \\in \\mathbb{R}^{H \\times V},\\quad\n",
    "b_2 \\in \\mathbb{R}^{V}.\n",
    "$$\n",
    "\n",
    "Forward pass:\n",
    "$$\n",
    "h = \\tanh(\\mathrm{cat}(E) W_1 + b_1) \\in \\mathbb{R}^{n \\times H},\n",
    "$$\n",
    "$$\n",
    "\\text{logits} = h W_2 + b_2 \\in \\mathbb{R}^{n \\times V}.\n",
    "$$\n",
    "\n",
    "### 3) Loss (cross-entropy)\n",
    "\n",
    "For a batch of size $n$, the training objective is the average cross-entropy loss:\n",
    "$$\n",
    "\\mathcal{L}(\\theta)\n",
    "=\n",
    "-\\frac{1}{n}\n",
    "\\sum_{k=1}^n\n",
    "\\log \\mathrm{softmax}(\\text{logits}_k)_{y_k}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## We will produce\n",
    "\n",
    "1. A dataset builder producing $(X, Y)$, where $X$ contains contexts of length $T$.\n",
    "2. An MLP language model implemented using raw PyTorch tensors.\n",
    "3. A training loop with minibatch stochastic gradient descent and a learning-rate schedule.\n",
    "4. Train / validation / test evaluation.\n",
    "5. A sampling procedure to generate new names.The full model can be summarized as:\n",
    "$$\n",
    "p_\\theta(y \\mid x)\n",
    "=\n",
    "\\mathrm{softmax}\\!\\Big(\n",
    "\\tanh\\!\\big(\n",
    "\\mathrm{cat}(C[x]) W_1 + b_1\n",
    "\\big)\n",
    "W_2 + b_2\n",
    "\\Big)_y.\n",
    "$$\n",
    "\n",
    "Equivalently, in arrow notation:\n",
    "$$\n",
    "x\n",
    "\\;\\xrightarrow{\\;C\\;}\n",
    "E\n",
    "\\;\\xrightarrow{\\;\\mathrm{concat}\\;}\n",
    "\\mathbb{R}^{Td}\n",
    "\\;\\xrightarrow{\\;W_1,b_1,\\tanh\\;}\n",
    "\\mathbb{R}^{H}\n",
    "\\;\\xrightarrow{\\;W_2,b_2\\;}\n",
    "\\mathbb{R}^{V}\n",
    "\\;\\xrightarrow{\\;\\mathrm{softmax}\\;}\n",
    "p_\\theta(\\cdot \\mid x).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ee19b-5611-400b-b712-de43abaeb2fa",
   "metadata": {},
   "source": [
    "The full model can be summarized as:\n",
    "$$\n",
    "x\n",
    "\\to C[x]\n",
    "\\to \\mathrm{cat}\n",
    "\\to \\tanh\n",
    "\\to \\mathrm{logits}\n",
    "\\to \\mathrm{softmax}\n",
    "\\to p_\\theta(\\cdot \\mid x),\n",
    "$$\n",
    "\n",
    "thus\n",
    "\n",
    "$$\n",
    "p_\\theta(y \\mid x)\n",
    "=\n",
    "\\mathrm{softmax}\\!\\Big(\n",
    "\\tanh\\!\\big(\n",
    "\\mathrm{cat}(C[x]) W_1 + b_1\n",
    "\\big)\n",
    "W_2 + b_2\n",
    "\\Big)_y.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56631840-254c-42e5-9f1b-a362f65f4603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running on google colab, run this cell\n",
    "%cd /content\n",
    "!rm -rf mini-course-DL\n",
    "!git clone https://github.com/AdamArras/mini-course-DL.git\n",
    "%cd mini-course-DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea26fc72-570f-42b0-934b-1c782ab2aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534dc7f",
   "metadata": {},
   "source": [
    "## 1) Setup, load data, build vocabulary (same as Part 1)\n",
    "\n",
    "**Exercise.**\n",
    "- Import libraries.\n",
    "- Load `names.txt` into `words`.\n",
    "- Build `stoi`, `itos`, `vocab_size`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8af0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: imports + load words + build stoi/itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d463748-031c-434c-8f2b-8b505ea0530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "32033\n",
      "min lenght = 2 and max lenght = 15\n"
     ]
    }
   ],
   "source": [
    "file = open('data/names.txt', 'r') \n",
    "words = file.read().splitlines() \n",
    "file.close() \n",
    "\n",
    "print(type(words))\n",
    "print(words[:10])\n",
    "print(len(words))\n",
    "print('min lenght = '+str(min([len(w) for w in words]))+ ' and max lenght = ' +str(max([len(w) for w in words])))\n",
    "\n",
    "alphabet = str('.abcdefghijklmnopqrstuvwxyz')\n",
    "vocab_size = len(alphabet) # this is V\n",
    "itos = {i:alphabet[i] for i in range(vocab_size)}\n",
    "stoi = {alphabet[i]:i for i in range(vocab_size)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494f9be",
   "metadata": {},
   "source": [
    "## 2) Build the dataset: contexts of length $T$\n",
    "\n",
    "We now construct the training dataset of **(context, next-character)** pairs.\n",
    "\n",
    "### Context length\n",
    "\n",
    "Choose a fixed context length:\n",
    "$$\n",
    "T = \\texttt{block\\_size} = 3.\n",
    "$$\n",
    "\n",
    "The model will predict the next character given the **previous $T$ characters**.\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical interpretation\n",
    "\n",
    "Each training example corresponds to:\n",
    "$$\n",
    "x_t = (i_{t-T}, \\dots, i_{t-1}) \\in \\{0,\\dots,V-1\\}^T,\n",
    "\\qquad\n",
    "y_t = i_t \\in \\{0,\\dots,V-1\\},\n",
    "$$\n",
    "with the convention:\n",
    "$$\n",
    "i_s = 0 \\quad \\text{for } s \\le 0,\n",
    "$$\n",
    "which implements left-padding using `\".\"`.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### How contexts are constructed (procedural view)\n",
    "\n",
    "For each word $w = w_1 w_2 \\dots w_L$ in the dataset:\n",
    "\n",
    "1. Initialize a context of length $T$ filled with zeros and append the boundary symbol `\".\"` to the word, and iterate over the characters:\n",
    "   $$\n",
    "   w_1, w_2, \\dots, w_L, \\texttt{\".\"}\n",
    "   $$\n",
    "\n",
    "3. At each step:\n",
    "   - Let $i = \\mathrm{stoi}(\\text{current character})$.\n",
    "   - Record one training example:\n",
    "     $$\n",
    "     x = \\text{current context}, \\qquad y = i.\n",
    "     $$\n",
    "   - Update the context by **dropping the oldest index and appending $i$**:\n",
    "     $$\n",
    "     \\text{context} \\leftarrow (\\text{context}[1:], i).\n",
    "     $$\n",
    "\n",
    "This sliding window procedure produces **one training pair per character**, including the final `\".\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### Output tensors\n",
    "\n",
    "After processing all words:\n",
    "\n",
    "- $X$ is a tensor of shape $(N, T)$ containing context indices,\n",
    "- $Y$ is a tensor of shape $(N,)$ containing target indices,\n",
    "- both must have dtype `torch.int64`.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise\n",
    "\n",
    "1. Implement a function `build_dataset(words_subset)` that:\n",
    "   - loops over words as described above,\n",
    "   - collects all contexts into a Python list `X`,\n",
    "   - collects all targets into a Python list `Y`,\n",
    "   - converts them to tensors.\n",
    "\n",
    "2. Verify:\n",
    "   ```python\n",
    "   X.shape == (N, T)\n",
    "   Y.shape == (N,)\n",
    "   X.dtype == torch.int64\n",
    "   Y.dtype == torch.int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5305faf-e950-40b5-a34d-b072a66dbe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement build_dataset(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16287e08-8e63-4779-994e-130e91a7d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words_list, block_size = 3):\n",
    "    X,Y = [],[]\n",
    "    \n",
    "    for name in words_list:\n",
    "        context = [0]*block_size\n",
    "        for ch in list(name) + ['.']:\n",
    "            it = stoi[ch]\n",
    "            #print(context,ch,it)\n",
    "            X.append(context)\n",
    "            Y.append(it)\n",
    "            context = context[1:] + [it]\n",
    "    \n",
    "    X = torch.tensor(X,dtype = torch.int64)\n",
    "    Y = torch.tensor(Y,dtype = torch.int64)\n",
    "\n",
    "    return(X,Y)\n",
    "# X,Y = build_dataset(words, block_size = 3)\n",
    "# X.shape, Y.shape\n",
    "# print(X[:15,:])\n",
    "# print(Y[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9e4f7",
   "metadata": {},
   "source": [
    "## 3) Train/dev/test split (by words)\n",
    "\n",
    "Shuffle `words` with a fixed seed and split 80/10/10 by word list indices.\n",
    "\n",
    "**Exercise.**\n",
    "- Create `(Xtr,Ytr)`, `(Xdev,Ydev)`, `(Xte,Yte)` using your `build_dataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df657f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: shuffle + split + build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94d99600-e6a1-432d-8d1e-43fcd8b9a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "X,Y = build_dataset(words)\n",
    "\n",
    "n_data = len(words)\n",
    "\n",
    "ntr = int(n_data*0.8)\n",
    "nte = int(n_data*0.9)\n",
    "\n",
    "Xtr,Ytr = build_dataset(words[:ntr])\n",
    "Xdev,Ydev  = build_dataset(words[ntr:nte])\n",
    "Xte,Yte  = build_dataset(words[nte:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e105ab",
   "metadata": {},
   "source": [
    "## 4) Initialize model parameters\n",
    "\n",
    "Choose hyperparameters:\n",
    "- embedding dim $d=10$\n",
    "- hidden width $H=200$\n",
    "- batch size $n=32$\n",
    "\n",
    "**Exercise.**\n",
    "Initialize:\n",
    "$$\n",
    "C\\in\\mathbb{R}^{V\\times d},\n",
    "\\quad\n",
    "W_1\\in\\mathbb{R}^{(Td)\\times H},\\ b_1\\in\\mathbb{R}^{H},\n",
    "\\quad\n",
    "W_2\\in\\mathbb{R}^{H\\times V},\\ b_2\\in\\mathbb{R}^{V}.\n",
    "$$\n",
    "Use a seeded `torch.Generator()` for reproducibility, and set `requires_grad=True` for all parameters.\n",
    "\n",
    "Also compute total parameter count:\n",
    "$$\n",
    "|C| + |W_1| + |b_1| + |W_2| + |b_2|.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "004da724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialize C, W1, b1, W2, b2 with a fixed generator seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7c71111-d28d-4735-9fcd-df2ba12be267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameter count = 11897\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "d_emb = 10\n",
    "block_size = 3\n",
    "H_hid = 200\n",
    "n_batch = 32\n",
    "\n",
    "C = torch.randn(vocab_size,d_emb, generator = g, requires_grad=True)\n",
    "W1 = torch.randn(block_size*d_emb , H_hid, generator = g, requires_grad=True) \n",
    "b1 = torch.randn(H_hid, generator = g, requires_grad=True)\n",
    "W2 = torch.randn(H_hid , vocab_size, generator = g, requires_grad=True) \n",
    "b2 = torch.randn(vocab_size, generator = g, requires_grad=True)\n",
    "\n",
    "parameters = [C,W1, b1, W2, b2]\n",
    "n_parameter = sum([param.nelement() for param in parameters])\n",
    "print('total parameter count = '+str(n_parameter) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa63fc9",
   "metadata": {},
   "source": [
    "## 5) Forward pass (minibatch)\n",
    "\n",
    "In practice, we do **not** use the entire dataset at each training step, as this would be too costly in terms of computation and memory. Instead, at each step we select a **random subset** of the dataset containing $n$ examples.  \n",
    "This subset is called a **minibatch** (of size n).\n",
    "\n",
    "A minibatch consists of:\n",
    "- input contexts $X_b \\in \\mathbb{Z}^{n \\times T}$,\n",
    "- target characters $Y_b \\in \\mathbb{Z}^{n}$.\n",
    "\n",
    "Each row of $X_b$ corresponds to one training example, and all $n$ examples are processed **in parallel**.\n",
    "\n",
    "Given a minibatch $X_b$, the forward pass computes the predictions and the loss as follows:\n",
    "\n",
    "1. **Embeddings**\n",
    "$$\n",
    "E = C[X_b] \\in \\mathbb{R}^{n \\times T \\times d}.\n",
    "$$\n",
    "\n",
    "2. **Concatenation**\n",
    "$$\n",
    "x = \\mathrm{reshape}(E) \\in \\mathbb{R}^{n \\times (T d)}.\n",
    "$$\n",
    "\n",
    "3. **Hidden layer**\n",
    "$$\n",
    "h = \\tanh(x W_1 + b_1) \\in \\mathbb{R}^{n \\times H}.\n",
    "$$\n",
    "\n",
    "4. **Logits**\n",
    "$$\n",
    "\\text{logits} = h W_2 + b_2 \\in \\mathbb{R}^{n \\times V}.\n",
    "$$\n",
    "\n",
    "5. **Loss**\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{NLL}(\\text{logits}, Y_b).\n",
    "$$\n",
    "\n",
    "The loss $\\mathcal{L}$ is a single scalar measuring how well the model predicts the targets for the minibatch.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise.**  \n",
    "Write a function `forward(Xb)` that returns `logits` and/or `loss`. In PyTorch, `F.cross_entropy()` implements softmax + NLL, you can use F.cross_entropy, after looking at the [documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9969f94-6c75-47c6-8f5a-9f3dc14cc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bdecf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement a forward pass for a minibatch\n",
    "# n_batch = 32\n",
    "\n",
    "# batch = torch.randint(len(Xtr),(n_batch,))\n",
    "\n",
    "# Xb = Xtr[batch]\n",
    "# Yb = Ytr[batch]\n",
    "\n",
    "def forward(X, Y=None):\n",
    "    n = X.shape[0]\n",
    "    E = C[X]\n",
    "    x = E.reshape(n, -1)\n",
    "    h = torch.tanh(x @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "\n",
    "    loss = None\n",
    "    if Y is not None:\n",
    "        loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "    return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ebf03d",
   "metadata": {},
   "source": [
    "## 6) Training loop: minibatch SGD\n",
    "\n",
    "Train for a large number of iteration $it = 2*10^5$. At each step:\n",
    "\n",
    "1. Sample minibatch indices $b \\in \\text{batch}$ .\n",
    "2. Forward pass to compute `loss`.\n",
    "3. Zero gradients.\n",
    "4. Backprop: `loss.backward()`.\n",
    "5. Update:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}.\n",
    "$$\n",
    "\n",
    "Learning rate schedule from the course:\n",
    "$$\n",
    "\\eta =\n",
    "\\begin{cases}\n",
    "0.1 & \\text{if } it < 10^5,\\\\\n",
    "0.01 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Track the $loss$ and plot it.\n",
    "\n",
    "**Exercise.**\n",
    "Implement the full training loop and produce the loss curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca4206-2bfe-4465-b1ad-512d8e5bae79",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "**Zero the gradients.**  \n",
    "In PyTorch, each parameter tensor `p` (with `p.requires_grad=True`) stores its gradient in `p.grad`.  \n",
    "When you call `loss.backward()`, PyTorch **adds** the newly computed gradient to `p.grad` (it *accumulates* gradients).  \n",
    "Therefore, at the beginning of each training step you must reset these stored gradients to zero, typically by doing:\n",
    "\n",
    "- `p.grad = None` (common and efficient), or `p.grad.zero_()`.\n",
    "\n",
    "If you forget to zero gradients, you will effectively sum gradients across steps, and the updates will be incorrect.\n",
    "\n",
    "---\n",
    "\n",
    "**Update the parameters (manual SGD).**  \n",
    "After `loss.backward()`, each parameter `p` has a gradient `p.grad`. A basic gradient descent update is: $\n",
    "p \\leftarrow p - \\eta \\, p.\\mathrm{grad}$, where $\\eta > 0$ is the **learning rate**. In this lesson we do not use a PyTorch optimizer yet (like `torch.optim.SGD`). Instead, we implement the update **manually** with the rule above, to make the mechanics explicit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1ece87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it =  0 loss =  23.44540786743164\n",
      "it =  20000 loss =  2.1942975521087646\n",
      "it =  40000 loss =  2.3334743976593018\n",
      "it =  60000 loss =  2.4005935192108154\n",
      "it =  80000 loss =  1.8847721815109253\n",
      "it =  100000 loss =  2.144014596939087\n",
      "it =  120000 loss =  2.398810625076294\n",
      "it =  140000 loss =  2.2233176231384277\n",
      "it =  160000 loss =  2.1608121395111084\n",
      "it =  180000 loss =  2.045356273651123\n"
     ]
    }
   ],
   "source": [
    "# TODO: training loop with lr schedule + loss tracking\n",
    "\n",
    "n_batch = 32\n",
    "\n",
    "it_list = []\n",
    "loss_list = []\n",
    "\n",
    "n_it = 200000\n",
    "\n",
    "for it in range(n_it):\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    # create a minibatch\n",
    "    batch = torch.randint(len(Xtr),(n_batch,))\n",
    "    Xb = Xtr[batch]\n",
    "    Yb = Ytr[batch]\n",
    "\n",
    "    #forward pass\n",
    "    logits, loss = forward(Xb, Yb)\n",
    "    \n",
    "    it_list.append(it)\n",
    "    loss_list.append(loss.item())\n",
    "    \n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #parameter update\n",
    "    lr = 0.1\n",
    "    if it>10**5:\n",
    "        lr = 0.01\n",
    "        \n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "    #track the loss\n",
    "    if it % 20000 == 0:\n",
    "        print('it = ', it, 'loss = ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8526e6f-ef3e-44f4-99d4-220b7bbb467f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x71db6ae5c140>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(it_list,loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb9037",
   "metadata": {},
   "source": [
    "## 7) Evaluate on train and dev splits\n",
    "\n",
    "Compute:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{train}} = \\mathrm{CE}(\\text{logits}(Xtr),Ytr),\n",
    "\\quad\n",
    "\\mathcal{L}_{\\text{dev}} = \\mathrm{CE}(\\text{logits}(Xdev),Ydev).\n",
    "$$\n",
    "\n",
    "**Exercise.**\n",
    "Write an evaluation function that runs the model on a full split (no batching needed if it fits in memory) and prints the loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "304cbd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1583, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1564, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# TODO: evaluate train and dev losses\n",
    "\n",
    "logits_dev, loss_dev = forward(Xdev, Ydev)\n",
    "\n",
    "print(loss_dev)\n",
    "\n",
    "logits, loss_te = forward(Xte, Yte)\n",
    "\n",
    "print(loss_te) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb708a1a",
   "metadata": {},
   "source": [
    "## 9) Sampling: generate names from the MLP\n",
    "\n",
    "Maintain a context of length $T$. Initialize with:\n",
    "\\[\n",
    "\\text{context} = (0,0,\\dots,0).\n",
    "\\]\n",
    "\n",
    "Iterate:\n",
    "1. Forward pass on the single context to get logits \\(\\in\\mathbb{R}^{1\\times V}\\).\n",
    "2. Convert to probabilities:\n",
    "$$\n",
    "p = \\mathrm{softmax}(\\text{logits}).\n",
    "$$\n",
    "3. Sample next index:\n",
    "$$\n",
    "i \\sim \\mathrm{Categorical}(p).\n",
    "$$\n",
    "4. Shift context and append \\(i\\).\n",
    "5. Stop when $i=0$ (`\".\"`).\n",
    "\n",
    "**Exercise.**\n",
    "Generate ~20 names with a fixed generator seed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0100fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sampling loop from the trained MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad5e9b9e-aea5-4211-8e2c-0f5429172415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacklony.\n",
      "nihan.\n",
      "dama.\n",
      "alette.\n",
      "navidah.\n",
      "jamarcoarleedo.\n",
      "mysh.\n",
      "kharleison.\n",
      "chy.\n",
      "plyn.\n",
      "erimone.\n",
      "keme.\n",
      "tee.\n",
      "alix.\n",
      "dessa.\n",
      "cai.\n",
      "afreem.\n",
      "lacer.\n",
      "minsleya.\n",
      "leawn.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for it in range(20):\n",
    "    \n",
    "    names = []\n",
    "    context = [0] * block_size\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "      E = C[torch.tensor([context])]\n",
    "      h = torch.tanh(E.view(1, -1) @ W1 + b1)\n",
    "      logits = h @ W2 + b2\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      names.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0249c",
   "metadata": {},
   "source": [
    "#### Question: I claim that the game changer in this architecture is the embeding matrix C. Do you agree?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nn-zero-to-hero)",
   "language": "python",
   "name": "nn-zero-to-hero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
