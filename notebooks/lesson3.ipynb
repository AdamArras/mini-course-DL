{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a265521e-aa86-4fe9-b86c-a488a921ef2f",
   "metadata": {},
   "source": [
    "# Lesson 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f6acd",
   "metadata": {},
   "source": [
    "**Goal.** We upgrade our previous model to a **fixed-context neural language model**\n",
    "(a neural $n$-gram). We increase the context length $T$, we introduce the embedding for characters and a multi-layer perceptron that predicts will the next character from the context.\n",
    "\n",
    "This lesson is based on Andrej Karpathyâ€™s *Neural Networks: Zero to Hero* video series  \n",
    "(https://karpathy.ai/), and follows the MLP architecture as introduced in the foundational paper  \n",
    "**Bengio et al. (2003)**, *A Neural Probabilistic Language Model*  \n",
    "(https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27222550-a7c8-4914-9add-2e4e60ff8e8e",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "- Vocabulary size: $V = 26 + 1$.\n",
    "- Context length: $T = 3$.\n",
    "- Embedding dimension: $d$.\n",
    "- Hidden width: $H$.\n",
    "\n",
    "A single training example consists of:\n",
    "$$\n",
    "x = (x_1,\\dots,x_T)\\in\\{0,\\dots,V-1\\}^T,\\qquad\n",
    "y\\in\\{0,\\dots,V-1\\}.\n",
    "$$\n",
    "\n",
    "We model the conditional distribution of the next character as:\n",
    "$$\n",
    "p_\\theta(y \\mid x)\n",
    "=\n",
    "\\mathrm{softmax}(\\text{logits}(x))_y.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture\n",
    "\n",
    "### 1) Embedding lookup\n",
    "\n",
    "Embedding table:\n",
    "$$\n",
    "C \\in \\mathbb{R}^{V \\times d}.\n",
    "$$\n",
    "\n",
    "For a batch of size $n$, that is, $X \\in \\mathbb{Z}^{n \\times T}$:\n",
    "$$\n",
    "E = C[X] \\in \\mathbb{R}^{n \\times T \\times d}.\n",
    "$$\n",
    "\n",
    "Concatenating the embeddings along the context dimension gives:\n",
    "$$\n",
    "\\mathrm{cat}(E) \\in \\mathbb{R}^{n \\times (T d)}.\n",
    "$$\n",
    "\n",
    "### 2) MLP\n",
    "\n",
    "Parameters:\n",
    "$$\n",
    "W_1 \\in \\mathbb{R}^{(T d) \\times H},\\quad\n",
    "b_1 \\in \\mathbb{R}^{H},\\qquad\n",
    "W_2 \\in \\mathbb{R}^{H \\times V},\\quad\n",
    "b_2 \\in \\mathbb{R}^{V}.\n",
    "$$\n",
    "\n",
    "Forward pass:\n",
    "$$\n",
    "h = \\tanh(\\mathrm{cat}(E) W_1 + b_1) \\in \\mathbb{R}^{n \\times H},\n",
    "$$\n",
    "$$\n",
    "\\text{logits} = h W_2 + b_2 \\in \\mathbb{R}^{n \\times V}.\n",
    "$$\n",
    "\n",
    "### 3) Loss (cross-entropy)\n",
    "\n",
    "For a batch of size $n$, the training objective is the average cross-entropy loss:\n",
    "$$\n",
    "\\mathcal{L}(\\theta)\n",
    "=\n",
    "-\\frac{1}{n}\n",
    "\\sum_{k=1}^n\n",
    "\\log \\mathrm{softmax}(\\text{logits}_k)_{y_k}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## We will produce\n",
    "\n",
    "1. A dataset builder producing $(X, Y)$, where $X$ contains contexts of length $T$.\n",
    "2. An MLP language model implemented using raw PyTorch tensors.\n",
    "3. A training loop with minibatch stochastic gradient descent and a learning-rate schedule.\n",
    "4. Train / validation / test evaluation.\n",
    "5. A sampling procedure to generate new names.The full model can be summarized as:\n",
    "$$\n",
    "p_\\theta(y \\mid x)\n",
    "=\n",
    "\\mathrm{softmax}\\!\\Big(\n",
    "\\tanh\\!\\big(\n",
    "\\mathrm{cat}(C[x]) W_1 + b_1\n",
    "\\big)\n",
    "W_2 + b_2\n",
    "\\Big)_y.\n",
    "$$\n",
    "\n",
    "Equivalently, in arrow notation:\n",
    "$$\n",
    "x\n",
    "\\;\\xrightarrow{\\;C\\;}\n",
    "E\n",
    "\\;\\xrightarrow{\\;\\mathrm{concat}\\;}\n",
    "\\mathbb{R}^{Td}\n",
    "\\;\\xrightarrow{\\;W_1,b_1,\\tanh\\;}\n",
    "\\mathbb{R}^{H}\n",
    "\\;\\xrightarrow{\\;W_2,b_2\\;}\n",
    "\\mathbb{R}^{V}\n",
    "\\;\\xrightarrow{\\;\\mathrm{softmax}\\;}\n",
    "p_\\theta(\\cdot \\mid x).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ee19b-5611-400b-b712-de43abaeb2fa",
   "metadata": {},
   "source": [
    "The full model can be summarized as:\n",
    "$$\n",
    "x\n",
    "\\to C[x]\n",
    "\\to \\mathrm{cat}\n",
    "\\to \\tanh\n",
    "\\to \\mathrm{logits}\n",
    "\\to \\mathrm{softmax}\n",
    "\\to p_\\theta(\\cdot \\mid x),\n",
    "$$\n",
    "\n",
    "thus\n",
    "\n",
    "$$\n",
    "p_\\theta(y \\mid x)\n",
    "=\n",
    "\\mathrm{softmax}\\!\\Big(\n",
    "\\tanh\\!\\big(\n",
    "\\mathrm{cat}(C[x]) W_1 + b_1\n",
    "\\big)\n",
    "W_2 + b_2\n",
    "\\Big)_y.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56631840-254c-42e5-9f1b-a362f65f4603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running on google colab, run this cell\n",
    "%cd /content\n",
    "!rm -rf mini-course-DL\n",
    "!git clone https://github.com/AdamArras/mini-course-DL.git\n",
    "%cd mini-course-DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea26fc72-570f-42b0-934b-1c782ab2aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534dc7f",
   "metadata": {},
   "source": [
    "## 1) Setup, load data, build vocabulary (same as Part 1)\n",
    "\n",
    "**Exercise.**\n",
    "- Import libraries.\n",
    "- Load `names.txt` into `words`.\n",
    "- Build `stoi`, `itos`, `vocab_size`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8af0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: imports + load words + build stoi/itos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494f9be",
   "metadata": {},
   "source": [
    "## 2) Build the dataset: contexts of length $T$\n",
    "\n",
    "We now construct the training dataset of **(context, next-character)** pairs.\n",
    "\n",
    "### Context length\n",
    "\n",
    "Choose a fixed context length:\n",
    "$$\n",
    "T = \\texttt{block\\_size} = 3.\n",
    "$$\n",
    "\n",
    "The model will predict the next character given the **previous $T$ characters**.\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical interpretation\n",
    "\n",
    "Each training example corresponds to:\n",
    "$$\n",
    "x_t = (i_{t-T}, \\dots, i_{t-1}) \\in \\{0,\\dots,V-1\\}^T,\n",
    "\\qquad\n",
    "y_t = i_t \\in \\{0,\\dots,V-1\\},\n",
    "$$\n",
    "with the convention:\n",
    "$$\n",
    "i_s = 0 \\quad \\text{for } s \\le 0,\n",
    "$$\n",
    "which implements left-padding using `\".\"`.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### How contexts are constructed (procedural view)\n",
    "\n",
    "For each word $w = w_1 w_2 \\dots w_L$ in the dataset:\n",
    "\n",
    "1. Initialize a context of length $T$ filled with zeros and append the boundary symbol `\".\"` to the word, and iterate over the characters:\n",
    "   $$\n",
    "   w_1, w_2, \\dots, w_L, \\texttt{\".\"}\n",
    "   $$\n",
    "\n",
    "3. At each step:\n",
    "   - Let $i = \\mathrm{stoi}(\\text{current character})$.\n",
    "   - Record one training example:\n",
    "     $$\n",
    "     x = \\text{current context}, \\qquad y = i.\n",
    "     $$\n",
    "   - Update the context by **dropping the oldest index and appending $i$**:\n",
    "     $$\n",
    "     \\text{context} \\leftarrow (\\text{context}[1:], i).\n",
    "     $$\n",
    "\n",
    "This sliding window procedure produces **one training pair per character**, including the final `\".\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### Output tensors\n",
    "\n",
    "After processing all words:\n",
    "\n",
    "- $X$ is a tensor of shape $(N, T)$ containing context indices,\n",
    "- $Y$ is a tensor of shape $(N,)$ containing target indices,\n",
    "- both must have dtype `torch.int64`.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise\n",
    "\n",
    "1. Implement a function `build_dataset(words_subset)` that:\n",
    "   - loops over words as described above,\n",
    "   - collects all contexts into a Python list `X`,\n",
    "   - collects all targets into a Python list `Y`,\n",
    "   - converts them to tensors.\n",
    "\n",
    "2. Verify:\n",
    "   ```python\n",
    "   X.shape == (N, T)\n",
    "   Y.shape == (N,)\n",
    "   X.dtype == torch.int64\n",
    "   Y.dtype == torch.int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5305faf-e950-40b5-a34d-b072a66dbe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement build_dataset(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9e4f7",
   "metadata": {},
   "source": [
    "## 3) Train/dev/test split (by words)\n",
    "\n",
    "Shuffle `words` with a fixed seed and split 80/10/10 by word list indices.\n",
    "\n",
    "**Exercise.**\n",
    "- Create `(Xtr,Ytr)`, `(Xdev,Ydev)`, `(Xte,Yte)` using your `build_dataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df657f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: shuffle + split + build datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e105ab",
   "metadata": {},
   "source": [
    "## 4) Initialize model parameters\n",
    "\n",
    "Choose hyperparameters:\n",
    "- embedding dim $d=10$\n",
    "- hidden width $H=200$\n",
    "- batch size $n=32$\n",
    "\n",
    "**Exercise.**\n",
    "Initialize:\n",
    "$$\n",
    "C\\in\\mathbb{R}^{V\\times d},\n",
    "\\quad\n",
    "W_1\\in\\mathbb{R}^{(Td)\\times H},\\ b_1\\in\\mathbb{R}^{H},\n",
    "\\quad\n",
    "W_2\\in\\mathbb{R}^{H\\times V},\\ b_2\\in\\mathbb{R}^{V}.\n",
    "$$\n",
    "Use a seeded `torch.Generator()` for reproducibility, and set `requires_grad=True` for all parameters.\n",
    "\n",
    "Also compute total parameter count:\n",
    "$$\n",
    "|C| + |W_1| + |b_1| + |W_2| + |b_2|.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "004da724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialize C, W1, b1, W2, b2 with a fixed generator seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa63fc9",
   "metadata": {},
   "source": [
    "## 5) Forward pass (minibatch)\n",
    "\n",
    "In practice, we do **not** use the entire dataset at each training step, as this would be too costly in terms of computation and memory. Instead, at each step we select a **random subset** of the dataset containing $n$ examples.  \n",
    "This subset is called a **minibatch** (of size n).\n",
    "\n",
    "A minibatch consists of:\n",
    "- input contexts $X_b \\in \\mathbb{Z}^{n \\times T}$,\n",
    "- target characters $Y_b \\in \\mathbb{Z}^{n}$.\n",
    "\n",
    "Each row of $X_b$ corresponds to one training example, and all $n$ examples are processed **in parallel**.\n",
    "\n",
    "Given a minibatch $X_b$, the forward pass computes the predictions and the loss as follows:\n",
    "\n",
    "1. **Embeddings**\n",
    "$$\n",
    "E = C[X_b] \\in \\mathbb{R}^{n \\times T \\times d}.\n",
    "$$\n",
    "\n",
    "2. **Concatenation**\n",
    "$$\n",
    "x = \\mathrm{reshape}(E) \\in \\mathbb{R}^{n \\times (T d)}.\n",
    "$$\n",
    "\n",
    "3. **Hidden layer**\n",
    "$$\n",
    "h = \\tanh(x W_1 + b_1) \\in \\mathbb{R}^{n \\times H}.\n",
    "$$\n",
    "\n",
    "4. **Logits**\n",
    "$$\n",
    "\\text{logits} = h W_2 + b_2 \\in \\mathbb{R}^{n \\times V}.\n",
    "$$\n",
    "\n",
    "5. **Loss**\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{NLL}(\\text{logits}, Y_b).\n",
    "$$\n",
    "\n",
    "The loss $\\mathcal{L}$ is a single scalar measuring how well the model predicts the targets for the minibatch.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise.**  \n",
    "Write a function `forward(Xb)` that returns `logits` and/or `loss`. In PyTorch, `F.cross_entropy()` implements softmax + NLL, you can use F.cross_entropy, after looking at the [documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9969f94-6c75-47c6-8f5a-9f3dc14cc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ef99a-93bb-42ff-b5de-a1dad3a3e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement a forward pass for a minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ebf03d",
   "metadata": {},
   "source": [
    "## 6) Training loop: minibatch SGD\n",
    "\n",
    "Train for a large number of iteration $it = 2*10^5$. At each step:\n",
    "\n",
    "1. Sample minibatch indices $b \\in \\text{batch}$ .\n",
    "2. Forward pass to compute `loss`.\n",
    "3. Zero gradients.\n",
    "4. Backprop: `loss.backward()`.\n",
    "5. Update:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}.\n",
    "$$\n",
    "\n",
    "Learning rate schedule from the course:\n",
    "$$\n",
    "\\eta =\n",
    "\\begin{cases}\n",
    "0.1 & \\text{if } it < 10^5,\\\\\n",
    "0.01 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Track the $loss$ and plot it.\n",
    "\n",
    "**Exercise.**\n",
    "Implement the full training loop and produce the loss curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca4206-2bfe-4465-b1ad-512d8e5bae79",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "**Zero the gradients.**  \n",
    "In PyTorch, each parameter tensor `p` (with `p.requires_grad=True`) stores its gradient in `p.grad`.  \n",
    "When you call `loss.backward()`, PyTorch **adds** the newly computed gradient to `p.grad` (it *accumulates* gradients).  \n",
    "Therefore, at the beginning of each training step you must reset these stored gradients to zero, typically by doing:\n",
    "\n",
    "- `p.grad = None` (common and efficient), or `p.grad.zero_()`.\n",
    "\n",
    "If you forget to zero gradients, you will effectively sum gradients across steps, and the updates will be incorrect.\n",
    "\n",
    "---\n",
    "\n",
    "**Update the parameters (manual SGD).**  \n",
    "After `loss.backward()`, each parameter `p` has a gradient `p.grad`. A basic gradient descent update is: $\n",
    "p \\leftarrow p - \\eta \\, p.\\mathrm{grad}$, where $\\eta > 0$ is the **learning rate**. In this lesson we do not use a PyTorch optimizer yet (like `torch.optim.SGD`). Instead, we implement the update **manually** with the rule above, to make the mechanics explicit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f01cd3-0eb6-4e8f-b077-532ce8cc539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: training loop with lr schedule + loss tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb9037",
   "metadata": {},
   "source": [
    "## 7) Evaluate on train and dev splits\n",
    "\n",
    "Compute:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{train}} = \\mathrm{CE}(\\text{logits}(Xtr),Ytr),\n",
    "\\quad\n",
    "\\mathcal{L}_{\\text{dev}} = \\mathrm{CE}(\\text{logits}(Xdev),Ydev).\n",
    "$$\n",
    "\n",
    "**Exercise.**\n",
    "Write an evaluation function that runs the model on a full split (no batching needed if it fits in memory) and prints the loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "304cbd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluate train and dev losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb708a1a",
   "metadata": {},
   "source": [
    "## 9) Sampling: generate names from the MLP\n",
    "\n",
    "Maintain a context of length $T$. Initialize with:\n",
    "\\[\n",
    "\\text{context} = (0,0,\\dots,0).\n",
    "\\]\n",
    "\n",
    "Iterate:\n",
    "1. Forward pass on the single context to get logits \\(\\in\\mathbb{R}^{1\\times V}\\).\n",
    "2. Convert to probabilities:\n",
    "$$\n",
    "p = \\mathrm{softmax}(\\text{logits}).\n",
    "$$\n",
    "3. Sample next index:\n",
    "$$\n",
    "i \\sim \\mathrm{Categorical}(p).\n",
    "$$\n",
    "4. Shift context and append \\(i\\).\n",
    "5. Stop when $i=0$ (`\".\"`).\n",
    "\n",
    "**Exercise.**\n",
    "Generate ~20 names with a fixed generator seed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0100fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sampling loop from the trained MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0249c",
   "metadata": {},
   "source": [
    "#### Question: I claim that the game changer in this architecture is the embeding matrix C. Do you agree?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nn-zero-to-hero)",
   "language": "python",
   "name": "nn-zero-to-hero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
